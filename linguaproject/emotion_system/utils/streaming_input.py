import os
import queue
import sounddevice as sd
import numpy as np
import threading
import tempfile
import wave

from faster_whisper import WhisperModel
from pyannote.audio import Pipeline

from ..emotion.text_emotion import classify_text_emotion
from ..emotion.audio_emotion import classify_audio_emotion
from ..features.extract_features import extract_features
from ..response.generate_response import generate_response
from logic_classify_system.risk_based_classifier import RiskScoreClassifier, ConsultationMetadata
HF_TOKEN = os.getenv("HF_TOKEN")

# ì˜¤ë””ì˜¤ í
audio_queue = queue.Queue()

# ëª¨ë¸ ì´ˆê¸°í™”
whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HF_TOKEN)
classifier = RiskScoreClassifier()


def audio_callback(indata, frames, time, status):
    audio_queue.put(indata.copy())


def save_temp_wav(audio_data, samplerate=16000):
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    with wave.open(temp_file.name, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(samplerate)
        wf.writeframes((audio_data * 32767).astype(np.int16).tobytes())
    return temp_file.name


def run_live_emotion_only():
    """ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰"""
    stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=16000)
    stream.start()

    def emotion_only_loop():
        while True:
            if not audio_queue.empty():
                audio_chunk = audio_queue.get()
                audio_data = np.squeeze(audio_chunk)
                segments, _ = whisper_model.transcribe(audio_data, language="ko")
                for segment in segments:
                    text = segment.text
                    temp_wav = save_temp_wav(audio_data)
                    features = extract_features(temp_wav)
                    text_emotion = classify_text_emotion(text)
                    audio_emotion = classify_audio_emotion(features)
                    final_emotion = text_emotion if text_emotion else audio_emotion
                    print(f"ë°œí™”: {text}")
                    print(f"ê°ì •: {final_emotion}")
                    print("-" * 50)

    threading.Thread(target=emotion_only_loop, daemon=True).start()
    print("ğŸ™ï¸ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("ğŸ›‘ ì‹¤ì‹œê°„ ì…ë ¥ ì¢…ë£Œ")
        stream.stop()


def run_live_emotion_with_diarization():
    """ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬"""
    stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=16000)
    stream.start()

    def emotion_diarization_loop():
        while True:
            if not audio_queue.empty():
                audio_chunk = audio_queue.get()
                audio_data = np.squeeze(audio_chunk)
                segments, _ = whisper_model.transcribe(audio_data, language="ko")
                diarization = diarization_pipeline(audio_data)
                for segment in segments:
                    text = segment.text
                    for turn, _, speaker in diarization.itertracks(yield_label=True):
                        temp_wav = save_temp_wav(audio_data)
                        features = extract_features(temp_wav)
                        text_emotion = classify_text_emotion(text)
                        audio_emotion = classify_audio_emotion(features)
                        final_emotion = text_emotion if text_emotion else audio_emotion
                        print(f"[{speaker}] ë°œí™”: {text}")
                        print(f"ê°ì •: {final_emotion}")
                        print("-" * 50)

    threading.Thread(target=emotion_diarization_loop, daemon=True).start()
    print("ğŸ™ï¸ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("ğŸ›‘ ì‹¤ì‹œê°„ ì…ë ¥ ì¢…ë£Œ")
        stream.stop()


def run_live_pipeline():
    """ì‹¤ì‹œê°„ ì „ì²´ íŒŒì´í”„ë¼ì¸"""
    stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=16000)
    stream.start()

    def full_loop():
        while True:
            if not audio_queue.empty():
                audio_chunk = audio_queue.get()
                audio_data = np.squeeze(audio_chunk)
                segments, _ = whisper_model.transcribe(audio_data, language="ko")
                diarization = diarization_pipeline(audio_data)
                for segment in segments:
                    text = segment.text
                    for turn, _, speaker in diarization.itertracks(yield_label=True):
                        print(f"[{speaker}] {text}")

                        # ìš•ì„¤ í•„í„°ë§
                        profanity_result = classifier.profanity_filter.filter_profanity(text)
                        if profanity_result:
                            print("ìš•ì„¤ ê°ì§€ â†’ CRITICAL ì²˜ë¦¬")
                            print("Risk Score:", profanity_result.risk_score, profanity_result.risk_level.name)
                            print("ê¶Œì¥ ì¡°ì¹˜:", profanity_result.recommendation)
                            print("-" * 50)
                            continue

                        # ê°ì • ë¶„ì„
                        temp_wav = save_temp_wav(audio_data)
                        features = extract_features(temp_wav)
                        text_emotion = classify_text_emotion(text)
                        audio_emotion = classify_audio_emotion(features)
                        final_emotion = text_emotion if text_emotion else audio_emotion

                        # Risk Score í‰ê°€
                        metadata = ConsultationMetadata(
                            consultation_content="ì‹¤ì‹œê°„ ìƒë‹´",
                            consultation_result="ì¶”ê°€ ìƒë‹´ í•„ìš”",
                            requirement_type="ë‹¨ì¼ ìš”ê±´",
                            consultation_reason="ì¼ë°˜"
                        )
                        risk_result = classifier.classify(text, metadata=metadata)

                        # ì‘ë‹µ ìƒì„±
                        response = generate_response(final_emotion, text)

                        # ì¶œë ¥
                        print(f"ê°ì •: {final_emotion}")
                        print(f"Risk Score: {risk_result.risk_score} ({risk_result.risk_level.name})")
                        print("ì‘ë‹µ:", response)
                        print("ê¶Œì¥ ì¡°ì¹˜:", risk_result.recommendation)
                        print("-" * 50)

    threading.Thread(target=full_loop, daemon=True).start()
    print("ğŸ™ï¸ ì‹¤ì‹œê°„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("ğŸ›‘ ì‹¤ì‹œê°„ ì…ë ¥ ì¢…ë£Œ")
        stream.stop()
