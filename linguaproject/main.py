import os
from emotion_system.diarization.speaker_split import diarize_and_transcribe
from emotion_system.emotion.text_emotion import classify_text_emotion
from emotion_system.emotion.audio_emotion import classify_audio_emotion
from emotion_system.features.extract_features import extract_features
from emotion_system.response.generate_response import generate_response
from emotion_system.response.compare_actions import compare_actions
from emotion_system.utils.audio_utils import convert_to_wav
from linguaproject.emotion_system.utils.streaming_input import (
    run_live_emotion_only,
    run_live_emotion_with_diarization,
    run_live_pipeline
)
from logic_classify_system.risk_based_classifier import RiskScoreClassifier
from logic_classify_system.classification_criteria import ConsultationMetadata

HF_TOKEN = os.getenv("HF_TOKEN")


def get_user_choice():
    print("ğŸ§ ìŒì„± ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ")
    print("2. ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥")
    input_mode = input("ì…ë ¥ ë²ˆí˜¸: ")

    print("\nğŸ§  ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("A. ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰")
    print("B. ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬")
    print("C. ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬ + Risk Score í‰ê°€")
    process_mode = input("ì…ë ¥ ë¬¸ì: ").upper()

    return input_mode, process_mode


def run_emotion_only(audio_path):
    print("\n[ê°ì • ë¶„ì„ë§Œ ìˆ˜í–‰]")
    # JSON ì €ì¥ë„ í•¨ê»˜ ìˆ˜í–‰
    segments = diarize_and_transcribe(audio_path, HF_TOKEN, save_json=True, json_path="emotion_only.json")

    for seg in segments:
        speaker = seg["speaker"]
        text = seg["text"]
        text_emotion = classify_text_emotion(text)
        features = extract_features(audio_path)
        audio_emotion = classify_audio_emotion(features)
        final_emotion = text_emotion if text_emotion else audio_emotion
        response = generate_response(final_emotion, text)

        print(f"[{speaker}] ë°œí™”: {text}")
        print(f"ê°ì •: {final_emotion}")
        print("ì‘ë‹µ:", response)
        print("-" * 50)


def run_emotion_with_diarization(audio_path):
    print("\n[ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬]")
    segments = diarize_and_transcribe(audio_path, HF_TOKEN, save_json=True, json_path="emotion_diarization.json")

    for seg in segments:
        speaker = seg["speaker"]
        text = seg["text"]
        text_emotion = classify_text_emotion(text)
        features = extract_features(audio_path)
        audio_emotion = classify_audio_emotion(features)
        final_emotion = text_emotion if text_emotion else audio_emotion

        print(f"[{speaker}] ë°œí™”: {text}")
        print(f"ê°ì •: {final_emotion}")
        print("-" * 50)


def run_full_pipeline(audio_path):
    print("\n[ê°ì • ë¶„ì„ + í™”ì ë¶„ë¦¬ + Risk Score í‰ê°€]")
    segments = diarize_and_transcribe(audio_path, HF_TOKEN, save_json=True, json_path="full_pipeline.json")
    classifier = RiskScoreClassifier()

    for seg in segments:
        speaker = seg["speaker"]
        text = seg["text"]

        # ìš•ì„¤ í•„í„°ë§
        profanity_result = classifier.profanity_filter.filter_profanity(text)
        if profanity_result:
            print(f"[{speaker}] ë°œí™”: {text}")
            print("ìš•ì„¤ ê°ì§€ â†’ CRITICAL ì²˜ë¦¬")
            print("Risk Score:", profanity_result.risk_score, profanity_result.risk_level.name)
            print("ê¶Œì¥ ì¡°ì¹˜:", profanity_result.recommendation)
            print("-" * 50)
            continue

        # ê°ì • ë¶„ì„
        text_emotion = classify_text_emotion(text)
        features = extract_features(audio_path)
        audio_emotion = classify_audio_emotion(features)
        final_emotion = text_emotion if text_emotion else audio_emotion

        # Risk Score í‰ê°€
        metadata = ConsultationMetadata(
            consultation_content="ê³ ì¶© ìƒë‹´",
            consultation_result="í•´ê²° ë¶ˆê°€",
            requirement_type="ë‹¤ìˆ˜ ìš”ê±´",
            consultation_reason="ì—…ì²´"
        )
        risk_result = classifier.classify(text, session_context=segments, metadata=metadata)

        # ìƒë‹´ì‚¬ ì‘ë‹µ ìƒì„±
        response = generate_response(final_emotion, text)

        # ê¶Œì¥ ì¡°ì¹˜ vs ì‹¤ì œ ì¡°ì¹˜ ë¹„êµ
        comparison = compare_actions(
            text,
            recommended_action="í™˜ë¶ˆ ì ‘ìˆ˜ í›„ 3ì¼ ë‚´ ì²˜ë¦¬",
            actual_action="ì²˜ë¦¬ ì§€ì—° ì¤‘"
        )

        print(f"[{speaker}] ë°œí™”: {text}")
        print(f"ê°ì •: {final_emotion}")
        print(f"Risk Score: {risk_result.risk_score} ({risk_result.risk_level.name})")
        print("ì‘ë‹µ:", response)
        print("ê¶Œì¥ ì¡°ì¹˜:", risk_result.recommendation)
        print("ì¡°ì¹˜ ë¹„êµ:", comparison)
        print("-" * 50)


def run_pipeline():
    input_mode, process_mode = get_user_choice()

    if input_mode == "1":
        audio_path = input("\nì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if audio_path.endswith((".m4a", ".mp3")):
            audio_path = convert_to_wav(audio_path)

        if process_mode == "A":
            run_emotion_only(audio_path)
        elif process_mode == "B":
            run_emotion_with_diarization(audio_path)
        elif process_mode == "C":
            run_full_pipeline(audio_path)
        else:
            print("âŒ ì˜ëª»ëœ ì²˜ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤.")

    elif input_mode == "2":
        if process_mode == "A":
            run_live_emotion_only()
        elif process_mode == "B":
            run_live_emotion_with_diarization()
        elif process_mode == "C":
            run_live_pipeline()
        else:
            print("âŒ ì˜ëª»ëœ ì²˜ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤.")
    else:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ ë°©ì‹ì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    run_pipeline()
