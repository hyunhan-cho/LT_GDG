"""
í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ë©”ì¸

ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í†µê³„ëŸ‰ì„ ë¬¸ì„œí™”
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ import
from logical_analysis.logic_classify_system.test.test_normal_label_classification import (
    test_normal_label_classification
)
from logical_analysis.logic_classify_system.test.test_special_label_classification import (
    test_special_label_classification
)
from logical_analysis.logic_classify_system.test.test_with_ground_truth import (
    test_with_ground_truth
)
from logical_analysis.logic_classify_system.test.test_feature_extraction import (
    test_feature_extraction,
    analyze_feature_extraction
)
from logical_analysis.logic_classify_system.test.test_statistics import (
    calculate_classification_metrics,
    calculate_validation_metrics,
    calculate_feature_extraction_metrics,
    export_metrics_to_json,
    export_metrics_to_markdown
)


def run_normal_label_test(
    data_dir: Path,
    output_dir: Path,
    sample_size: int = 500,
    verbose: bool = True
) -> Optional[Dict[str, Any]]:
    """
    Normal Label ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        data_dir: STT ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        sample_size: ìƒ˜í”Œ í¬ê¸°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í…ŒìŠ¤íŠ¸ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
    """
    if not data_dir.exists():
        if verbose:
            print(f"[ê±´ë„ˆëœ€] ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return None
    
    if verbose:
        print("\n" + "=" * 80)
        print("í…ŒìŠ¤íŠ¸ 1: Normal Label ë¶„ë¥˜ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
    
    try:
        stats = test_normal_label_classification(
            data_dir=data_dir,
            max_files=None,
            sample_size=sample_size
        )
        
        if stats:
            # í†µê³„ëŸ‰ ê³„ì‚°
            metrics = calculate_classification_metrics(stats)
            
            # JSON ì €ì¥
            json_path = output_dir / 'metrics' / 'normal_label_metrics_v2.json'
            export_metrics_to_json(metrics, json_path)
            
            # Markdown ì €ì¥
            md_path = output_dir / 'metrics' / 'normal_label_metrics_v2.md'
            export_metrics_to_markdown(metrics, md_path, "Normal Label ë¶„ë¥˜ í†µê³„ëŸ‰ ë³´ê³ ì„œ (v2)")
            
            return {
                'test_name': 'normal_label_classification',
                'stats': stats,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
    except Exception as e:
        if verbose:
            print(f"[ì˜¤ë¥˜] Normal Label í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None
    
    return None


def run_special_label_test(
    data_dir: Path,
    output_dir: Path,
    sample_size: int = 500,
    verbose: bool = True
) -> Optional[Dict[str, Any]]:
    """
    Special Label ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        data_dir: STT ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        sample_size: ìƒ˜í”Œ í¬ê¸°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í…ŒìŠ¤íŠ¸ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
    """
    if not data_dir.exists():
        if verbose:
            print(f"[ê±´ë„ˆëœ€] ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return None
    
    if verbose:
        print("\n" + "=" * 80)
        print("í…ŒìŠ¤íŠ¸ 2: Special Label ë¶„ë¥˜ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
    
    try:
        stats = test_special_label_classification(
            data_dir=data_dir,
            max_files=None,
            sample_size=sample_size
        )
        
        if stats:
            # í†µê³„ëŸ‰ ê³„ì‚°
            metrics = calculate_classification_metrics(stats)
            
            # JSON ì €ì¥
            json_path = output_dir / 'metrics' / 'special_label_metrics_v2.json'
            export_metrics_to_json(metrics, json_path)
            
            # Markdown ì €ì¥
            md_path = output_dir / 'metrics' / 'special_label_metrics_v2.md'
            export_metrics_to_markdown(metrics, md_path, "Special Label ë¶„ë¥˜ í†µê³„ëŸ‰ ë³´ê³ ì„œ (v2)")
            
            return {
                'test_name': 'special_label_classification',
                'stats': stats,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
    except Exception as e:
        if verbose:
            print(f"[ì˜¤ë¥˜] Special Label í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None
    
    return None


def run_ground_truth_test(
    talksets_file: Path,
    output_dir: Path,
    sample_size: int = 500,
    verbose: bool = True
) -> Optional[Dict[str, Any]]:
    """
    ì •ë‹µì§€ ê¸°ë°˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        talksets_file: talksets ì›ë³¸ íŒŒì¼
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        sample_size: ìƒ˜í”Œ í¬ê¸°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í…ŒìŠ¤íŠ¸ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
    """
    if not talksets_file.exists():
        if verbose:
            print(f"[ê±´ë„ˆëœ€] talksets íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {talksets_file}")
        return None
    
    if verbose:
        print("\n" + "=" * 80)
        print("í…ŒìŠ¤íŠ¸ 3: ì •ë‹µì§€ ê¸°ë°˜ ê²€ì¦ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
    
    try:
        from logical_analysis.logic_classify_system.test.test_with_ground_truth import (
            create_ground_truth_dataset,
            validate_with_ground_truth,
            print_validation_results
        )
        
        # ì •ë‹µì§€ ë°ì´í„°ì…‹ ìƒì„±
        gt_output_dir = output_dir / 'ground_truth_validation'
        stt_data_list, ground_truth_list = create_ground_truth_dataset(
            talksets_file=talksets_file,
            sample_size=sample_size,
            output_dir=gt_output_dir
        )
        
        if not stt_data_list or not ground_truth_list:
            if verbose:
                print("[ì˜¤ë¥˜] ì •ë‹µì§€ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
            return None
        
        # ê²€ì¦ ì‹¤í–‰
        results = validate_with_ground_truth(stt_data_list, ground_truth_list)
        
        if verbose:
            print_validation_results(results)
        
        if results:
            # í†µê³„ëŸ‰ ê³„ì‚°
            metrics = calculate_validation_metrics(results)
            
            # JSON ì €ì¥
            json_path = output_dir / 'metrics' / 'ground_truth_metrics_v2.json'
            export_metrics_to_json(metrics, json_path)
            
            # Markdown ì €ì¥
            md_path = output_dir / 'metrics' / 'ground_truth_metrics_v2.md'
            export_metrics_to_markdown(metrics, md_path, "ì •ë‹µì§€ ê¸°ë°˜ ê²€ì¦ í†µê³„ëŸ‰ ë³´ê³ ì„œ (v2)")
            
            return {
                'test_name': 'ground_truth_validation',
                'results': results,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
    except Exception as e:
        if verbose:
            print(f"[ì˜¤ë¥˜] ì •ë‹µì§€ ê¸°ë°˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    return None


def run_feature_extraction_test(
    data_dir: Path,
    output_dir: Path,
    sample_size: int = 200,
    verbose: bool = True
) -> Optional[Dict[str, Any]]:
    """
    íŠ¹ì§•ì  ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        data_dir: STT ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        sample_size: ìƒ˜í”Œ í¬ê¸°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í…ŒìŠ¤íŠ¸ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
    """
    if not data_dir.exists():
        if verbose:
            print(f"[ê±´ë„ˆëœ€] ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return None
    
    if verbose:
        print("\n" + "=" * 80)
        print("í…ŒìŠ¤íŠ¸ 4: íŠ¹ì§•ì  ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
        print("=" * 80)
    
    try:
        from logical_analysis.logic_classify_system.test.test_feature_extraction import (
            load_stt_file,
            analyze_feature_extraction
        )
        from logical_analysis.logic_classify_system.pipeline.main_pipeline import MainPipeline
        
        # STT íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        stt_files = sorted(data_dir.glob('*.json'))
        
        if not stt_files:
            if verbose:
                print(f"[ì˜¤ë¥˜] STT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
            return None
        
        # ìƒ˜í”Œë§
        if len(stt_files) > sample_size:
            import random
            stt_files = random.sample(stt_files, sample_size)
        
        # MainPipeline ì´ˆê¸°í™”
        pipeline = MainPipeline()
        
        # ê²°ê³¼ ì €ì¥
        all_results = []
        processed_files = 0
        
        if verbose:
            print(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(stt_files)}")
            print("íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        
        for i, stt_file in enumerate(stt_files, 1):
            try:
                stt_data = load_stt_file(stt_file)
                result = pipeline.process(stt_data)
                all_results.append(result)
                processed_files += 1
                
                if verbose and i % 50 == 0:
                    print(f"  ì§„í–‰ ìƒí™©: {i}/{len(stt_files)} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            except Exception as e:
                if verbose and processed_files == 0:
                    print(f"  [ì˜¤ë¥˜] ì˜¤ë¥˜ ë°œìƒ ({stt_file.name}): {e}")
                continue
        
        if not all_results:
            if verbose:
                print("[ì˜¤ë¥˜] ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # íŠ¹ì§•ì  ì¶”ì¶œ ë¶„ì„
        stats = analyze_feature_extraction(all_results)
        
        if stats:
            # í†µê³„ëŸ‰ ê³„ì‚°
            metrics = calculate_feature_extraction_metrics(stats)
            
            # JSON ì €ì¥
            json_path = output_dir / 'metrics' / 'feature_extraction_metrics_v2.json'
            export_metrics_to_json(metrics, json_path)
            
            # Markdown ì €ì¥
            md_path = output_dir / 'metrics' / 'feature_extraction_metrics_v2.md'
            export_metrics_to_markdown(metrics, md_path, "íŠ¹ì§•ì  ì¶”ì¶œ í†µê³„ëŸ‰ ë³´ê³ ì„œ (v2)")
            
            return {
                'test_name': 'feature_extraction',
                'stats': stats,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
    except Exception as e:
        if verbose:
            print(f"[ì˜¤ë¥˜] íŠ¹ì§•ì  ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    return None


def run_all_tests(
    script_dir: Path,
    output_dir: Path = None,
    sample_sizes: Dict[str, int] = None,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        script_dir: ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ (í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ê³³)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ script_dir/test_results ì‚¬ìš©)
        sample_sizes: ê° í…ŒìŠ¤íŠ¸ë³„ ìƒ˜í”Œ í¬ê¸°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if output_dir is None:
        output_dir = script_dir / 'test_results'
    
    if sample_sizes is None:
        sample_sizes = {
            'normal': 500,
            'special': 500,
            'ground_truth': 500,
            'feature_extraction': 200
        }
    
    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / 'metrics').mkdir(parents=True, exist_ok=True)
    
    if verbose:
        print("=" * 80)
        print("ì „ì²´ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        print("=" * 80)
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print()
    
    all_results = {
        'timestamp': datetime.now().isoformat(),
        'tests': {},
        'summary': {}
    }
    
    # í…ŒìŠ¤íŠ¸ 1: Normal Label ë¶„ë¥˜
    normal_data_dir = script_dir / 'temp_extract_stt'
    normal_result = run_normal_label_test(
        data_dir=normal_data_dir,
        output_dir=output_dir,
        sample_size=sample_sizes.get('normal', 500),
        verbose=verbose
    )
    if normal_result:
        all_results['tests']['normal_label_classification'] = normal_result
    
    # í…ŒìŠ¤íŠ¸ 2: Special Label ë¶„ë¥˜
    special_data_dir = script_dir / 'talksets_stt'
    special_result = run_special_label_test(
        data_dir=special_data_dir,
        output_dir=output_dir,
        sample_size=sample_sizes.get('special', 500),
        verbose=verbose
    )
    if special_result:
        all_results['tests']['special_label_classification'] = special_result
    
    # í…ŒìŠ¤íŠ¸ 3: ì •ë‹µì§€ ê¸°ë°˜ ê²€ì¦
    talksets_file = script_dir / 'talksets-train-6.json'
    ground_truth_result = run_ground_truth_test(
        talksets_file=talksets_file,
        output_dir=output_dir,
        sample_size=sample_sizes.get('ground_truth', 500),
        verbose=verbose
    )
    if ground_truth_result:
        all_results['tests']['ground_truth_validation'] = ground_truth_result
    
    # í…ŒìŠ¤íŠ¸ 4: íŠ¹ì§•ì  ì¶”ì¶œ
    feature_data_dir = script_dir / 'talksets_stt'  # ë˜ëŠ” ë‹¤ë¥¸ ë°ì´í„°ì…‹
    feature_result = run_feature_extraction_test(
        data_dir=feature_data_dir,
        output_dir=output_dir,
        sample_size=sample_sizes.get('feature_extraction', 200),
        verbose=verbose
    )
    if feature_result:
        all_results['tests']['feature_extraction'] = feature_result
    
    # ìš”ì•½ í†µê³„ ìƒì„±
    if verbose:
        print("\n" + "=" * 80)
        print("í…ŒìŠ¤íŠ¸ ìš”ì•½")
        print("=" * 80)
    
    summary = {
        'total_tests': len(all_results['tests']),
        'completed_tests': list(all_results['tests'].keys()),
        'test_results': {}
    }
    
    for test_name, test_result in all_results['tests'].items():
        if 'metrics' in test_result:
            metrics = test_result['metrics']
            
            # ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
            if 'overall_accuracy' in metrics:
                summary['test_results'][test_name] = {
                    'overall_accuracy': metrics['overall_accuracy'],
                    'overall_error_rate': metrics.get('overall_error_rate', 0)
                }
            elif 'normal_ratio' in metrics:
                summary['test_results'][test_name] = {
                    'normal_ratio': metrics['normal_ratio'],
                    'special_ratio': metrics.get('special_ratio', 0)
                }
    
    all_results['summary'] = summary
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    results_path = output_dir / 'all_test_results_v2.json'
    export_metrics_to_json(all_results, results_path)
    
    # ìš”ì•½ Markdown ìƒì„±
    summary_md_path = output_dir / 'test_summary_v2.md'
    summary_lines = [
        "# ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½ (v2)",
        "",
        f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**ë²„ì „**: v2 (Special Label ìš”ì¸ í•©ì‚° ë°©ì‹ ì ìš©)",
        "",
        "---",
        "",
        "## ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½",
        "",
        f"- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {summary['total_tests']}ê°œ",
        f"- **ì™„ë£Œëœ í…ŒìŠ¤íŠ¸**: {', '.join(summary['completed_tests'])}",
        "",
        "---",
        "",
        "## ğŸ“ˆ í…ŒìŠ¤íŠ¸ë³„ ì£¼ìš” ì§€í‘œ",
        ""
    ]
    
    for test_name, test_metrics in summary['test_results'].items():
        summary_lines.append(f"### {test_name}")
        summary_lines.append("")
        for key, value in test_metrics.items():
            if isinstance(value, float):
                summary_lines.append(f"- **{key}**: {value:.2f}%")
            else:
                summary_lines.append(f"- **{key}**: {value}")
        summary_lines.append("")
    
    summary_lines.extend([
        "---",
        "",
        "## ğŸ“ ìƒì„¸ ê²°ê³¼",
        "",
        "ê° í…ŒìŠ¤íŠ¸ì˜ ìƒì„¸ í†µê³„ëŸ‰ì€ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¸ì¡°í•˜ì„¸ìš”:",
        "",
        "- `metrics/normal_label_metrics_v2.json`",
        "- `metrics/special_label_metrics_v2.json`",
        "- `metrics/ground_truth_metrics_v2.json`",
        "- `metrics/feature_extraction_metrics_v2.json`",
        "",
        "ì „ì²´ ê²°ê³¼: `all_test_results_v2.json`",
        "",
        "---",
        "",
        "## ğŸ”„ ì£¼ìš” ë³€ê²½ì‚¬í•­ (v2)",
        "",
        "- **Special Label ì‹ ë¢°ë„ ê³„ì‚° ë°©ì‹ ë³€ê²½**: korcen + baseline ê·œì¹™ ìš”ì¸ë“¤ì„ í•©ì‚°í•˜ì—¬ ì‹ ë¢°ë„ ê³„ì‚°",
        "- **Normal Label ì‹ ë¢°ë„ ì œê±°**: ì •ìƒ ë°œí™”ë¡œ íŒë‹¨í•˜ê²Œ ëœ ê·¼ê±°ë¥¼ ì •ëŸ‰í™”í•˜ê¸° ì–´ë ¤ì›Œ ì œê±°",
        "- **Special Label ìš”ì¸ë³„ ì ìˆ˜ ì¶”ê°€**: `special_label_confidence`ì™€ ê° ìš”ì¸ë³„ ê¸°ì—¬ë„(`*_factor_score`) ì œê³µ",
        "- **ìš”ì¸ ê°œìˆ˜ ê°€ì¤‘ì¹˜**: Special Label ìš”ì¸ì´ ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ìƒìŠ¹",
        ""
    ])
    
    with open(summary_md_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary_lines))
    
    if verbose:
        print(f"\n[ì™„ë£Œ] ì „ì²´ ê²°ê³¼ ì €ì¥: {results_path}")
        print(f"[ì™„ë£Œ] ìš”ì•½ ë¬¸ì„œ ì €ì¥: {summary_md_path}")
    
    return all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    script_dir = Path(__file__).parent
    output_dir = script_dir / 'test_results'
    
    # ìƒ˜í”Œ í¬ê¸° ì„¤ì •
    sample_sizes = {
        'normal': 500,
        'special': 500,
        'ground_truth': 500,
        'feature_extraction': 200
    }
    
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = run_all_tests(
        script_dir=script_dir,
        output_dir=output_dir,
        sample_sizes=sample_sizes,
        verbose=True
    )
    
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
    print("=" * 80)


if __name__ == "__main__":
    main()

