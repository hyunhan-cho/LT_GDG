'''
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline # Pipeline í´ë˜ìŠ¤ ì‚¬ìš© ê¶Œì¥ (3.1 ì´ìƒ)
# pyannote.core.Annotation í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# pyannote.coreëŠ” Pyannote ì„¤ì¹˜ ì‹œ í•¨ê»˜ ì„¤ì¹˜ë©ë‹ˆë‹¤.
try:
    from pyannote.core import Annotation
except ImportError:
    # ì´ ì˜ˆì™¸ëŠ” pyannote.coreê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤.
    print("âš ï¸ [Import Error] 'pyannote.core' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install pyannote.core ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    # ì„ì‹œì ìœ¼ë¡œ Annotationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë„ë¡ ì²˜ë¦¬
    class Annotation:
        def __init__(self, *args, **kwargs):
            raise NotImplementedError("pyannote.core.Annotation is not available.")

from dotenv import load_dotenv
import os
import json
from huggingface_hub import login
# torchaudio ëŒ€ì‹  soundfileê³¼ torchë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë¡œë”©ì„ ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
import soundfile as sf 
try:
    import torch
except ImportError:
    torch = None
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ğŸš€ ë³€ê²½ë¨: ìµœì‹  Pyannote 4.x ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ëŠ” 3.1 ëª¨ë¸ ì‚¬ìš©
REPO_ID = "pyannote/speaker-diarization-3.1"

# âš ï¸ ë””ë²„ê¹… ë¼ì¸
print(f"[Debug] HF_TOKEN is loaded: {bool(HF_TOKEN)}")
print(f"[Debug] HF_TOKEN prefix: {HF_TOKEN[:8] if HF_TOKEN else 'None'}")

# Hugging Face ëª…ì‹œì  ë¡œê·¸ì¸
if HF_TOKEN:
    try:
        login(token=HF_TOKEN, add_to_git_credential=False)
        print("âœ… [HF Auth] Hugging Face í† í°ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‹œì  ë¡œê·¸ì¸ ì„±ê³µ.")
    except Exception as e:
        print(f"âŒ [HF Auth] ëª…ì‹œì  ë¡œê·¸ì¸ ì‹¤íŒ¨. ì›ì¸: {e}")
else:
    print("âŒ [HF Auth] HF_TOKEN ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

# Whisper ëª¨ë¸ ì´ˆê¸°í™” (CPU í™˜ê²½ì—ì„œëŠ” small ê¶Œì¥)
whisper_model = WhisperModel("small", device="cpu", compute_type="int8")

# í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
def get_diarization_pipeline():
    if not HF_TOKEN:
        raise ValueError("HF_TOKEN is not set in environment.")

    try:
        print(f"ğŸ”„ [Pyannote] ì›ê²© ë¦¬í¬ì§€í† ë¦¬ ({REPO_ID})ì—ì„œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹œë„...")
        
        # Pyannote 3.1+ / 4.x ë²„ì „ì—ì„œëŠ” `use_auth_token` ì¸ìˆ˜ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
        pipeline = Pipeline.from_pretrained(REPO_ID)
        
        return pipeline
    except Exception as e:
        print(f"âŒ Pyannote íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨.")
        print(f"   ì›ì¸: {e}")
        print("   ğŸ‘‰ ì¤‘ìš”: 'pyannote/speaker-diarization-3.1' ë° ì˜ì¡´ì„± ëª¨ë¸ë“¤ì˜ ì•½ê´€ì— ë™ì˜í–ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”!")
        raise e

def diarize_and_transcribe(audio_path, save_json=False, json_path="segments.json"):
    # ğŸ¤ STT ì‹¤í–‰
    print(f"ğŸ¤ [STT] Whisper ëª¨ë¸ë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
    segments, info = whisper_model.transcribe(audio_path, language="ko")

    print(f"ğŸ‘¥ [Diarization] í™”ì ë¶„ë¦¬ ì‹œì‘...")
    try:
        diarization_pipeline = get_diarization_pipeline()

        # ì˜¤ë””ì˜¤ ë¡œë“œ ë° í…ì„œ ë³€í™˜
        waveform_numpy, sample_rate = sf.read(audio_path, dtype='float32')
        if waveform_numpy.ndim == 1:
            waveform_tensor = torch.from_numpy(waveform_numpy[np.newaxis, :])
        else:
            waveform_tensor = torch.from_numpy(waveform_numpy).T
        input_audio = {"waveform": waveform_tensor, "sample_rate": sample_rate}

        diarization = diarization_pipeline(input_audio)

    except Exception as e:
        print(f"âŒ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    # ğŸš¨ DEBUGGING
    print("\n=======================================================")
    print(f"DEBUG: Diarization ê°ì²´ íƒ€ì…: {type(diarization)}")
    print(f"DEBUG: Annotation ì†ì„± íƒ€ì…: {type(diarization.annotation)}")
    print(f"DEBUG: .itertracks ì¡´ì¬ ì—¬ë¶€: {hasattr(diarization.annotation, 'itertracks')}")
    print("=======================================================\n")

    print(f"ğŸ”— [Merging] STT ê²°ê³¼ì™€ í™”ì ì •ë³´ ë³‘í•© ì‹œì‘...")

    results = []
    try:
        annotation = diarization.annotation  # í•µì‹¬ ë³€ê²½

        for segment, _, speaker_label in annotation.itertracks(yield_label=True):
            start_time = segment.start
            end_time = segment.end

            matched_texts = [
                seg.text for seg in segments
                if seg.start < end_time and seg.end > start_time
            ]
            text = " ".join(matched_texts).strip()

            results.append({
                "speaker": speaker_label,
                "start": start_time,
                "end": end_time,
                "text": text
            })

        print(f"âœ… [Merging Success] ì´ {len(results)}ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì™„ë£Œ.")

    except Exception as e:
        print(f"âŒ í™”ì ë¶„ë¦¬ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ìµœì¢… ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    if save_json:
        print(f"ğŸ’¾ [Save] ê²°ê³¼ë¥¼ {json_path}ì— ì €ì¥í•©ë‹ˆë‹¤.")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)

    return results

'''

from faster_whisper import WhisperModel
import json

# Whisper ëª¨ë¸ ì´ˆê¸°í™” (CPU í™˜ê²½ì—ì„œëŠ” small ê¶Œì¥)
whisper_model = WhisperModel("small", device="cpu", compute_type="int8")

def transcribe_with_timestamps(audio_path, save_json=False, json_path="segments.json"):
    # ğŸ¤ Whisper STT ì‹¤í–‰
    segments, info = whisper_model.transcribe(audio_path, language="ko")

    results = []
    for seg in segments:
        results.append({
            "start": seg.start,
            "end": seg.end,
            "text": seg.text.strip()
        })

    # ğŸ’¾ JSON ì €ì¥ ì˜µì…˜
    if save_json:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)

    return results
